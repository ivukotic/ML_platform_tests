{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "To harness to full abilities of distributed high throughput computing on the OSG, it is necessary to learn how to scale up and control large numbers of jobs. This requires the ability to submit and process multiple jobs in parallel. Examples of workloads that require these considerations include multi-dimensional Monte Carlo integration using sampling, parameter sweep(s) for a given model, and molecular dynamics simulation with several initial conditions. All of these workloads require submitting more than a handful of jobs. \n",
    "\n",
    "![fig 1](https://raw.githubusercontent.com/SWC-OSG-Workshop/OSG-UserTraining-RMACC17/gh-pages/novice/DHTC/Images/htc_vs_hpc_serial.png)\n",
    "\n",
    "The HTCondor submit file keyword `queue` can run multiple jobs from a single job description file. In this tutorial, we will see how to scale up the calculations for a simple Python example using the `queue` keyword.\n",
    "\n",
    "Once we understand the basic HTCondor script to run a single job, it is easy to scale up.\n",
    "\n",
    "You can get the example files via the `tutorial` command,\n",
    "\n",
    "    $ tutorial ScalingUp-Python\n",
    "    $ cd tutorial-ScalingUp-Python\n",
    "\n",
    "The `tutorial-ScalingUp-Python` directory contains all the required files. This includes the sample Python program, job description file, and executable files. \n",
    "\n",
    "## Python Script\n",
    "\n",
    "Here, we are going to use a brute force approach to finding the minimum/maximum (also known as \"optimiziation\") of a two dimensional function on a grid of points. Let us take a look at the function (also known as the objective function) that we are trying to optimize:\n",
    "\n",
    "    f = (1 - x)**2 + (y - x**2)**2\n",
    "\n",
    "This is the two dimensional Rosenbrock function, which is used to test the robustness of an optimization method. \n",
    "\n",
    "By default, Python script will randomly select the boundary values of the grid that the optimization procedure will scan over. These values can be overridden by user supplied values.\n",
    "\n",
    "![fig 2](https://raw.githubusercontent.com/OSGConnect/tutorial-ScalingUp-Python/master/Images/RosenBrockFunction.png)\n",
    "\n",
    "To run the calculation with random boundary values, the script is executed without any argument\n",
    "    \n",
    "    $ module load python/3.4\n",
    "    $ module load all-pkgs\n",
    "    $ python rosen_brock_brute_opt.py\n",
    "    \n",
    "To run the calculation with the user-supplied boundary values, the script is executed with input arguments\n",
    "\n",
    "    python rosen_brock_brute_opt.py x_low x_high y_low y_high\n",
    "\n",
    "where `x_low` and `x_high` are low and high values along x-direction, and `y_low` and `y_high` are the low and high values along the y-direction.\n",
    "\n",
    "For example, to set the boundary in the x-direction as (-3, 3) and the boundary in the y-direction as (-2, 2), run\n",
    "\n",
    "    $ python rosen_brock_brute_opt.py -3 3 -2 2\n",
    "    \n",
    "\n",
    "The directory `Example1` runs the Python script with the default random values. The directories `Example2`, `Example3` and `Example4` deal with supplying the boundary values as input arguments. \n",
    "\n",
    "## Execution Script\n",
    "\n",
    "Let us take a look at the execution script, `cat scalingup-python-wrapper.sh`\n",
    "\n",
    "    #!/bin/bash\n",
    "\n",
    "    module load python/3.4\n",
    "    module load all-pkgs\n",
    "\n",
    "    python ./rosen_brock_brute_opt.py  $1 $2 $3 $4\n",
    "\n",
    "The wrapper loads the the relevant modules and then executes the python script `rosen_brock_brute_opt.py`. The python script takes four optional arguments.\n",
    "\n",
    "## Submitting Set of Jobs with Single Submit File\n",
    "\n",
    "![fig 3](https://raw.githubusercontent.com/SWC-OSG-Workshop/OSG-UserTraining-RMACC17/gh-pages/novice/DHTC/Images/queue_N_command.png)\n",
    "\n",
    "Now let us take a look at job description file \n",
    "\n",
    "    $ cd Example1\n",
    "    $ cat ScalingUp-PythonCals.submit\n",
    "\n",
    "If we want to submit several jobs, we need to track log, out and error files for each job. An easy way to do this is to add the `$(Cluster)` and `$(Process)` variables to the file names. \n",
    "\n",
    "    # The UNIVERSE defines an execution environment. You will almost always use VANILLA.\n",
    "    Universe = vanilla\n",
    "\n",
    "    # These are good base requirements for your jobs on the OSG. It is specific on OS and\n",
    "    # OS version, core, and memory, and wants to use the software modules. \n",
    "    Requirements = OSGVO_OS_STRING == \"RHEL 6\" && TARGET.Arch == \"X86_64\" && HAS_MODULES == True \n",
    "    request_cpus = 1\n",
    "    request_memory = 1 GB\n",
    "\n",
    "    # executable is the program your job will run \n",
    "    # It's often useful to create a shell script to \"wrap\" your actual work.\n",
    "    executable = ../scalingup-python-wrapper.sh \n",
    "\n",
    "    # files transferred into the job sandbox\n",
    "    transfer_input_files = ../rosen_brock_brute_opt.py\n",
    "\n",
    "    # error and output are the error and output channels from your job\n",
    "    # that HTCondor returns from the remote host.\n",
    "    output = Log/job.out.$(Cluster).$(Process)\n",
    "    error = Log/job.error.$(Cluster).$(Process)\n",
    "\n",
    "\n",
    "    # The log file is where HTCondor places information about your\n",
    "    # job's status, success, and resource consumption.\n",
    "    log = Log/job.log.$(Cluster).$(Process)\n",
    "\n",
    "    # Send the job to Held state on failure. \n",
    "    on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)  \n",
    "\n",
    "    # Periodically retry the jobs every 60 seconds, up to a maximum of 5 retries. \n",
    "    # The RANDOM_INTEGER(60, 600, 120) means random integers are generated between \n",
    "    # 60 and 600 seconds with a step size of 120 seconds. The failed jobs are \n",
    "    # randomly released with a spread of 1-10 minutes.  Releasing multiple jobs at \n",
    "    # the same time causes stress for the login node, so the random spread is a \n",
    "    # good approach to periodically release the failed jobs. \n",
    "\n",
    "    PeriodicRelease = ( (CurrentTime - EnteredCurrentStatus) > $RANDOM_INTEGER(60, 600, 120) ) && ((NumJobStarts < 5))\n",
    "\n",
    "    # Queue is the \"start button\" - it launches any jobs that have been\n",
    "    # specified thus far.\n",
    "    queue 10\n",
    "\n",
    "Note the `queue 10`. This tells HTCondor to queue 10 copies of this job under one cluster id.  \n",
    "\n",
    "Let us submit the above job\n",
    "\n",
    "    $ condor_submit ScalingUp-PythonCals.submit\n",
    "    Submitting job(s)..........\n",
    "    10 job(s) submitted to cluster 329837.\n",
    "\n",
    "Apply your `condor_q` and `watch` (`watch -n2 condor_q $USER`) knowledge to see this job progress . After all jobs finished, execute the `post_process.sh` script to sort the results. \n",
    "\n",
    "    ./post_process.sh\n",
    "\n",
    "<!-- ## Other ways to use `queue` command\n",
    "\n",
    "Now we will explore other ways to use `queue` command. In the previous example, we did not pass any argument to the program and it used randomly-generated boundary conditions. If we have some idea about where the minimum/maximum is, we can supply boundary conditions to the calculation through arguments. In our example, the minimum  of the Rosenbrock function is located at (1,1).\n",
    "\n",
    "### Supply multiple arguments via queue command\n",
    "\n",
    "![fig 4](https://raw.githubusercontent.com/OSGConnect/tutorial-ScalingUp-Python/master/Images/Slide3.png)\n",
    "\n",
    "We can still use a slightly modified version of job description file from the previous example to supply multiple arguments. The modified job description file is available in the `Example2` directory.  Move into that directory and take look at the end of job description file `ScalingUp-PythonCals.submit`:  \n",
    "\n",
    "    $ cd ../Example2\n",
    "    $ cat  ScalingUp-PythonCals.submit\n",
    "    \n",
    "    [...]\n",
    "    #Supply arguments \n",
    "    arguments = -9 9 -9 9\n",
    "\n",
    "    # Queue is the \"start button\" - it launches any jobs that have been\n",
    "    # specified thus far.\n",
    "    queue\n",
    "\n",
    "    arguments = -8 8 -8 8\n",
    "    queue \n",
    "\n",
    "    arguments = -7 7 -7 7\n",
    "    queue \n",
    "    [...]\n",
    "\n",
    "A major part of the job description file looks the same as the previous example. The main difference is that the addition of `arguments` keyword. Each time the queue command appears in the script, the expression(s) before the queue would be added to the job description. \n",
    "\n",
    "Let us submit the above job\n",
    "\n",
    "    $ condor_submit ScalingUp-PythonCals.submit\n",
    "    Submitting job(s)..........\n",
    "    9 job(s) submitted to cluster 329838.\n",
    "\n",
    "Apply your `condor_q` and `connect watch` knowledge to see this job progress. After all jobs finished, execute the `post_process.sh`  script to sort the results. \n",
    "    ./post_process.sh\n",
    " -->\n",
    "\n",
    "## Variable Expansion via `queue` Keyword\n",
    "\n",
    "![fig 5](https://raw.githubusercontent.com/SWC-OSG-Workshop/OSG-UserTraining-RMACC17/gh-pages/novice/DHTC/Images/queue_arg_set.png)\n",
    "\n",
    "In the previous example, we did not pass any argument to the program and it used randomly-generated boundary conditions. If we have some idea about where the minimum/maximum is, we can supply boundary conditions to the calculation through arguments. The `arguments` keyword allows us to pass command line arguments to the `executable`. To get a better idea of what are good boundary conditions, we want to scan over a number of boundary parameter sets. The `queue` command allows us to submit a job for each possible boundary condition we want to test using a single submit file. \n",
    "\n",
    "Take a look at the job description file in Example3. \n",
    "\n",
    "    $ cd ../Example3\n",
    "    $ cat ScalingUp-PythonCals.submit\n",
    "    [...]\n",
    "    queue arguments from (\n",
    "    -9 9 -9 9 \n",
    "    -8 8 -8 8 \n",
    "    -7 7 -7 7 \n",
    "    -6 6 -6 6 \n",
    "    -5 5 -5 5 \n",
    "    -4 4 -4 4 \n",
    "    -3 3 -3 3 \n",
    "    -2 2 -2 2 \n",
    "    -1 1 -1 1 \n",
    "    )\n",
    "\n",
    "Let us submit the above job\n",
    "\n",
    "    $ condor_submit ScalingUp-PythonCals.submit\n",
    "    Submitting job(s)..........\n",
    "    9 job(s) submitted to cluster 329839.\n",
    "\n",
    "Apply your `condor_q` and `watch` knowledge to see this job progress. After all jobs finished, execute the `post_process.sh`  script to sort the results. \n",
    "\n",
    "    ./post_process.sh\n",
    "\n",
    "In fact, we could define variables and assign them to HTCondor's expression. This is illustrated in Example 4. \n",
    "\n",
    "    $ cd ../Example4\n",
    "    $ cat ScalingUp-PythonCals.submit\n",
    "\n",
    "    [...]\n",
    "    # arguments keyword\n",
    "    arguments = $(x_low) $(x_high) $(y_low) $(y_high)\n",
    "\n",
    "    # queue keyword  \n",
    "    queue x_low x_high y_low y_high from (\n",
    "    -9 9 -9 9 \n",
    "    -8 8 -8 8 \n",
    "    -7 7 -7 7 \n",
    "    -6 6 -6 6 \n",
    "    -5 5 -5 5 \n",
    "    -4 4 -4 4 \n",
    "    -3 3 -3 3 \n",
    "    -2 2 -2 2 \n",
    "    -1 1 -1 1 \n",
    "    )\n",
    "\n",
    "The `queue` command defines the variables `x_low`, `x_high`, `y_low`, and `y_high`. These variables are passed on to the argument command (`arguments = $(x_low) $(x_high) $(y_low) $(y_high)`). \n",
    " \n",
    "Let us submit the above job\n",
    "\n",
    "    $ condor_submit ScalingUp-PythonCals.submit\n",
    "    Submitting job(s)..........\n",
    "    9 job(s) submitted to cluster 329840.\n",
    "\n",
    "Apply your `condor_q` and `watch` knowledge to see this job progress. After all jobs finished, execute the `post_process.sh`  script to sort the results. \n",
    "\n",
    "    ./post_process.sh\n",
    "\n",
    "<!-- ## Beyond the `queue` Keyword\n",
    "\n",
    "For larger workloads, with more than 100 jobs or job interdependencies, there are a couple tools (DAGMan and Pegasus) that we recommend using. They are beyond the scope of this tutorial. Please contact our support staff for more details. \n",
    "\n",
    " <div class=\"keypoints\" markdown=\"1\">\n",
    "#### Key Points\n",
    "\n",
    "<h2> Key Points </h2>\n",
    "* Scaling up the computational resources on OSG is crucial to taking full advantage of grid computing.\n",
    "* Changing the value of `queue` allows the user to scale up the resources.\n",
    "* `Arguments` allows you to pass parameters to a job script.\n",
    "* `$(Cluster)` and `$(Process)` can be used to name log files uniquely.\n",
    "* Check the HTCondor manual to learn more about the `queue` command (https://research.cs.wisc.edu/htcondor/manual/latest/2_5Submitting_Job.html).\n",
    "</div> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
